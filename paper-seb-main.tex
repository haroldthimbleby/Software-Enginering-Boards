\documentclass{comjnl}
\usepackage{ragged2e}
\usepackage{amsmath}
\usepackage{amssymb}

%----
  \setlength\textheight{220mm}
%----

\input paper-seb-macros.tex

\long\def\ignore#1{}

%\linenumbers

\title{\mytitle} 
\author{Harold Thimbleby}
\shortauthors{Harold Thimbleby}
\affiliation{See Change Fellow in Digital Health}

%\subject{Computational science (bioinformatics,~medical~computing,~etc)}

\keywords{Computational Science; Software Engineering; Reproducibility; Scientific scrutiny; Reproducible Analytic Pathway\raise 1ex\hbox{$\star$} (\RAPstar)}

\received{30 April 2022}
%\revised{--}

\email{harold@thimbleby.net}

%\begin{tabular}{@{}ll}Current Address: &62 Cyncoed Road, Cardiff, CF23 5SH, Wales\\
%&\texttt{harold@thimbleby.net}\\
%&ORCID 0000-0003-2222-4243
%\end{tabular}

%\orcid{0000-0003-2222-4243}

\begin{document}

\begin{abstract}
\justifying
\noindent
While data, analysis, and scientific models are described in papers and are peer reviewed, the algorithms and code generating insights and results often avoid scrutiny, and therefore the details of many scientific conclusions cannot be rigorously justified. Data is worthless unless the code works correctly. 

\hskip 1em This paper shows that scientists rarely assure the structure and quality of code they rely on, rarely make full code available for wider use or scrutiny, and rarely provide adequate documentation to understand or use their code reliably. This paper therefore justifies and proposes ways to mature the computational sciences:

%\emph{Problems:} Assumptions in scientific code are hard to scrutinize as they are rarely made explicit, even when code is made available. Both algorithms and code have bugs, unknown and accidental assumptions that have unwanted effects. Code is fallible, so any interpretation that relies on code is also fallible. When the code is not clearly structured and published with adequate documentation, the code cannot be usefully scrutinized. In turn, scientific claims cannot be properly scrutinized.

%\emph{Methodology:} From the perspective of Software Engineering, this paper critiques the quality (and accessibility) of coding in computational science, particularly as relied on in leading COVID-19 pandemic research driving international public health interventions. 
%The paper reports a pilot survey of peer reviewed computational modeling papers ($N=\the\dataN$) published in leading scientific journals.

%Code can be improved using software engineering. This paper argues for specific solutions:

%\emph{Results:} %Journals do not always have a code policy, and many have a relaxed approach to code disclosure. 

%\noindent\emph{Solutions:} 

\raggedright
\newdimen \mywidth \mywidth=\textwidth
\advance \mywidth by -3em

\def\indented#1{\vskip 1ex
\parshape 2 0em \textwidth 2em \mywidth 
\noindent 
\hbox to 2em{ #1.\hfill}\ignorespaces}

\indented 1
Professional software engineers can help and should be involved, particularly in critical research such as public health, climate, etc; 

\indented 2
``Software Engineering Boards'' (analogous to Ethics or Institutional Review Boards) should be instigated and used; 

\indented 3
Code, when used, should be considered an intrinsic part of any publication, and therefore should be formally reviewed by competent software engineers. 

\indented 4
The effective and increasingly popular Reproducible Analytic Pathway (RAP) methodology should be generalized to cover code and Software Engineering methodologies, in a generalization this paper terms \RAPstarp. Furthermore \RAPstar\ should be supported and encouraged in journal, conference, and funding body policies.

\vskip 1ex 
\noindent
The \supplement\ for this paper provides a summary of professional Software Engineering best practice relevant to scientific research and publication. It also includes suggestions for \RAPstar\ processes, and a pilot survey of code quality in leading peer-reviewed journals that corroborates the concerns of the paper.

\begin{quote}\def\seprule{\vskip 2ex\hrule\vskip 2ex}
\seprule

\def\zq{\setbox0=\hbox{``}\hskip -\wd0\copy0}
\zq Science is what we understand well enough to explain to a computer.''\\
\hfill {\rm Donald E Knuth in $A=B$ \cite{a=b}}\\

\zq Criticism is the mother of methodology.''\\
\hfill {\rm Robert P Abelson in \emph{Statistics as Principled Argument\/} \cite{abelson}}

\seprule
\end{quote}

\end{abstract}

\maketitle

%\section*{Author summary}
%
%%\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{HT}}]
%Harold Thimbleby PhD, FRCP (Edinburgh), Hon.\ FRSA, Hon.\ FRCP is See Change Fellow in Digital Health at Swansea University, Wales. His research focuses on human error and computer system design, particularly for healthcare. 
%%In addition to over 340 peer reviewed and 188 invited publications, Harold has written several books, including \emph{Press On\/} (MIT Press, 2007), which was winner of the American Association of Publishers best book in computer science award.
%Harold won the British Computer Society Wilkes Medal. He is emeritus Gresham Professor of Geometry (a chair founded in 1597), and has been a Royal Society-Leverhulme Trust Senior Research Fellow and a Royal Society-Wolfson Research Merit Award holder. His latest book is  \emph{Fix IT: How to see and solve the problems of digital healthcare\/} \cite{fixit}.  See his web site, \texttt{www.harold.thimbleby.net}, for more details.

\section{Introduction}\label{problems}
The discoveries and inventions of technologies like microscopes, telescopes, and X-rays, drive and expand the sciences. There are fascinating periods as new technologies and science unify; for example, thermometers could not measure temperature until the science was mature, and the science could not be ready until there was consensus in scientific methodologies, but the consensus in turn depended on reproducible thermometer measurements and a thorough understanding of the science, including all the confounding factors that had been misunderstood \cite{temperature}. 

Computers are a unique technology, far more flexible and challenging than thermometers, that not only expand science's horizons and support new discoveries, but they can also \emph{do\/} new science --- and with AI and other increasingly routine techniques they can do both real and speculative investigations better than humans. Computational methods are a distinctive and extraordinarily powerful, very flexible technology for scientists far more radical in their impact on science than conventional technologies. 

``Computational science'' has come to mean a particular style of science, based on using computational models, but, really, \emph{all\/} of science is now computational. Computational science is not just restricted to computational chemistry, genomics, or big data~\ldots\ in all fields of science, computation is used at every step, from note taking, literature searches, correspondence with co-authors and editors, through to typesetting finished publications.

\subsection{The statistics/computation analogy}
The central role of computational methods in science may be fruitfully compared to statistics, an established scientific tool --- and also, of course, a substantial field of research in its own right. 

Poor statistics is much easier to do than good statistics, and there are many examples of science being let down by na\"\i vely planned and poorly implemented statistics. Often scientists do not realize the limitations of their own statistical skills, so careful scientists generally work closely with professional statisticians.

{I}{n good science}, all statistics, methods and results have to be reported very carefully and in precise detail. For example, a statistical claim might be summarized as follows:

\begin{quote}\raggedright
\setbox0=\hbox{``}
\hskip -\wd0\box0 %Random intercept linear mixed model suggesting significant time by intervention-arm interaction effect. 
\ldots\ Bonferroni adjusted estimated mean difference between intervention-arms at 8-weeks 2.52 $( 95\%$ CI $0.78, 4.27, p = 0.0009)$. Between group effect size $d = 0.55$ $(95\%$ CI $0.32,$ $0.77)$.'' \cite{example-stats}
\end{quote}

This typical wording briefly summarizes confidence intervals, $p$ levels, and so on, to present statistical results so the paper's claims can be seen to be complete, easy to interpret, and easy to scrutinize. It is a \emph{lingua franca}. It may look technical, but it is written in the standard and now widely accepted form for summarizing statistics. Moreover, behind any such brief paragraph is a substantial, rigorous, and appropriate statistical analysis.

Scientists write like this and conferences and journals require it because statistical claims need to be properly accountable and documented in a clear way. Speigelhalter \cite{Speigelhalter} says statistical information needs to be accessible, intelligible, assessable, and usable; he also suggests probing questions to help assess statistical quality (see \supplement\ section~\ref{supplementary-Speigelhalter-section}). Results should not be uncritically accepted just because they are claimed.

The skill and effort required to do statistics so it can be communicated clearly and correctly, as above, is not to be taken for granted. Scientists work closely with competent, often specialist, statisticians who engage with the research from experiment design through to analysis and publication. Further, it is assumed that statistics will be peer reviewed, and that review will often lead to improvement. 

Scientists accept that statistics is a distinct, professional science, itself subject of research and continual improvement. Among other implications of the depth and progress of the field of statistics, undergraduate statistics options for general scientists are insufficient training for rigorous work in science --- their main value, perhaps, is to help scientists to understand the value of collaborating with specialist statisticians. Collaboration with statisticians is particularly important when new types of work are undertaken, where the statistical pitfalls have not already been well-explored.

Except in the most trivial of cases, all numbers and graphs, along with the statistics underlying them will be generated by computer. Indeed, computers are now very widely used, not just to calculate statistics, but to run the models (the term is defined below), do the data sampling and processing, and even to operate the sensors that generate the data that is analyzed. Computers are used to run human-participant surveys, such as web-based surveys. The data --- including the databases and bibliographic sources --- and code to analyze it is all stored and manipulated on computers. Computers even help with the word processing and typesetting of research.

In short, computers, data, and computer code are central to modern science. However, using code raises many critical questions: formats, backup, cyber-vulnerability, version control, integrity checking (e.g., managing human error), auditing, debugging and testing, and more. As with statistics, good answers to such ``technical'' issues makes the science that relies on them better. Software code, like statistics, is also subject to unintentional bias \cite{Ben,se-bias}.  All these issues are non-trivial concerns requiring technical expertise to manage well. 

A common oversight in scientific papers is to present a model, such as a set of differential equations, but omit how that model is transformed into code that generates the results the paper summarizes; if so, the code may have problems that cannot be identified as there is no specification to reference it to.

Failure to properly document and explain computer code undermines the scientific value of the models and the results they generate, in the same way as failure to properly articulate statistics undermines the value of any scientific claims. Indeed, as few papers use code that is as well-understood and as well-researched as standard statistical procedures (such as Bonferroni corrections), the scientific problems of poorly reported code are widespread. 

%When relevant details of models are omitted (or are not described in recognisable ways), it is impossible to properly scrutinise claims. Describing a model used, perhaps mathematically, or providing the code (e.g., to download) that implements it is as inadequate as just providing raw data without details of the statistics analysis supporting the claim. 

We would not believe a statistical claim that was obtained from some \emph{ad hoc\/} analysis with a new fangled method devised just for one project --- instead, we demand statistics that is recognizable, even traditional, so we are assured we understand what has been done and how reliable results were obtained. An interesting overlap with statistical and software engineering sloppiness is the many papers that just disclose as part of their methodology that they used a particular statistical package (e.g., ``Data analyses were performed using SAS 9.2 (SAS Institute, Cary, North Carolina, USA),'' yet \emph{how\/} those analyses might have been performed is not discussed. The problem is that admitting using SAS 9.2 or any other named system does not help scrutiny, as such systems can do anything with the data. A reviewer, if nobody else, needs to actually examine the statistical code itself and its documentation to assess whether the analysis presented in the paper is appropriate and suffciently reliable. 

It is recognized that to make critical claims, models need to be run under varying assumptions \cite{whitty}, yet somehow it is easy to overlook that the code that implements the models also needs to be carefully tested under varying assumptions to uncover and fix bugs and biases. Being able to understand (at least in principle) the exact code used in implementing a model is critical to having confidence in the results that rely on it. Code is rarely considered a substantial part of the science to which it contributes. 

This paper reviews a selection of papers in leading international journals, and finds that both papers and journal policies take code for granted. This paper then argues that, just as is routine for statistics, code and results from code (and the data it is run on) need to be discussed and presented in a way that properly assure belief in any claims derived from using them. Specifically, code should be developed and discussed in a sufficiently professional, rigorous, and recognizable way that is able to support clear presentation and scrutiny. Developing justifiably reliable code is the concern of the field of \emph{software engineering}, which will be discussed further below, as well as more substantially in this paper's \supplement). 

This paper shows that unreliable computational dependencies in science are widespread. Furthermore, code is rarely published in any useful form or professionally scrutinized, and therefore the code itself does not contribute to furthering reliable science, for instance through replication or reproduction. In short, the quality of much modern science seems to be undermined because the code it relies on is rarely of adequate quality \emph{for the uses to which it is put}. 

This paper explores the extent of these software engineering problems in published science. The paper additionally suggests some ways forward. The \supplement\ is an integral part of the suggested solutions. In particular, the \supplement\ section~\ref{supplementary-Speigelhalter-section} summarizes Speigelhater's uncontroversial statistics probes and draws explicit analogies for the critical assessment of the quality of scientific code.

\subsection{The role of code in science and scientific publication}
%Yet without code, models could not be run. 

For the purposes of this paper, models map theory and parameters to describe phenomena, typically to make predictions or to test and refine the models. With the possible exception of theoretical research, all but the simplest models require computers to evaluate; indeed even theoretical mathematics is now routinely performed by computer systems.

Whereas the mathematical form of a model may be concise and readily explained, even a basic computational representation of a model can easily run to thousands of lines of code, and its parameters --- its data --- may also be extensive. The chance that a thousand lines of hand-written code is error free is negligible, and therefore good practice demands that checks and constraints should be applied to improve its reliability. How to do this well is the concern of software engineering, and is discussed throughout this paper. 

While scientific research may rely on relatively easily-scrutinized mathematical models, or models that seem in principle easy to mathematize, the models that are run on computers to obtain the results published are sometimes not disclosed, and even when they are (certainly in all cases reviewed later in this paper) they are long, complex, inscrutable and (our survey shows) lack adequate documentation. Therefore the models are very likely to be unreliable \emph{in principle}. If code is not well-documented, this is not only a problem for reviewers and scientists reading the research to understand the intention of the code, but it also causes problems for the original researchers themselves: how can they understand its thinking well enough (e.g., a few weeks or months later) to maintain it correctly if has not been clearly documented? Without documentation, including a reasoned case to assure that the approach taken is sound \cite{assurance-case}, how do researchers, let alone reviewers, know exactly what they are doing?

Without substantial documentation it is impossible to scrutinize code properly. Consider just the single line ``\texttt{y = k*exp(x)}'' where there can be \emph{no\/} concept of its correctness \emph{unless\/} there is also an explicitly stated relation between the code and the mathematical specifications. What does it mean? What does \texttt{k} mean --- is it a basic constant or the result of some previous complex calculation? Does the code mean what was intended? What are the assumptions on \texttt{k}, \texttt{x}, and \texttt{y}, and do they hold invariantly? Moreover, as code generally consists of thousands of such lines, with numerous inter-dependencies, plus calling on many complex libraries of support code, it is inevitable that the \emph{collective\/} meaning will be unknown. A good programer would (in the example here) at least check that \texttt{k} and \texttt{x} are in range and that \texttt{k*exp} was behaving as expected (e.g., in case of under- or overflow).

Without explicit links to the relevant models (typically mathematics, depending on the claims), it is impossible to reason whether any code is correct, and in turn it is impossible to scientifically scrutinize results obtained from using the code. Not providing code and documentation, providing partial code, or providing code without the associated reasoning is analogous to claiming ``statistical results are significant'' without any discussion of the relevant methods and statistical details that justify making such a claim. If such an unjustified pseudo-statistical claim was made in a scientific paper, a reviewer would be justified in asking whether a competent experiment had even been performed. It would be generous to ask the author to provide the missing details so the paper could be better reviewed on resubmission. 

Contrary to the views expressed in the present paper, some authors have asserted that the purpose of code is to provide insight into models, rather than precise (generally numerical) analyses summarizing data \cite{assessing-quality} --- code can also be used to analyze and critique scientific theories directly. If code is inadequate, ``insights'' it provides will be flawed, and flawed in unquantified and unknown ways. Indeed, none of the papers sampled (described in section~\ref{survey-section}) claimed their papers were using code for insight; all papers claimed, explicitly or implicitly, that their code outputs were integral to their peer reviewed results.

Clearly, like statistics, programming (coding) can be done poorly and reported poorly, or it can be done well and reported well --- and any mix between these extremes. The question is whether it matters, \emph{when\/} it matters, and if so when it does, \emph{what\/} can be done to \emph{appropriately\/} help improve the quality of code (and discussions about the code) in scientific work.

%This paper explores the role of code in scientific research. Initially focusing on examples from pandemic modelling (because of its relevance to serious matters of public health) this paper shows that scientific culture, including editorial processes, have not adapted to the increasingly dominant role of code and managing the risks of its fallibility. The paper then suggests how to improve, including building mutual engagement across the science and software engineering communities.

\subsection{\RAPstar s: Generalized reproducible analytical pipelines}\label{RAP-section}
Writing a paper typically starts in a word processor (such as Microsoft Word), sketching an outline, writing boiler-plate text (such as the authors' names and standard section headings), and then gradually building up the evidence base (including citing the literature) that the paper relies on. The process will be concurrent with many other activities --- grant writing, writing up lab books, negotiating authorship, protecting IP, workshops, finding publication outlets, and so on.

The simplified diagram in figure \ref{fig-pipeline} illustrates the core pipeline of how experiments and data are used to provide information on which analysis and calculations are based, the results of which are then edited into the paper.

\begin{figure*}[t]
\begin{center}
\def\drop#1{\setbox0=\hbox{\lower .5em\hbox{#1}}%
\ht0=0em
\dp0=0em
\copy0}
\def\lowarrow{\drop{$\rightarrow$}}

\def\bigBracket{$
	\setbox0=\hbox{\lower 1.2ex\hbox{$\left.\vbox to 4em{\vfill}\right\}$}}
	\ht0=0em \dp0=0em 
	~\copy0
$}

\begin{tabular}{|lcl@{}cl@{\hskip 2em}cl|} \hline
\drop{\bf Data sources}&\lowarrow&\bf \drop{Analysis} & \lowarrow&\bf Select results& \lowarrow& \bf Submit for\\
\bf &&\bf & &\bf for write up & & \bf publication \\ \hline \hline 
&&&&&&\\
Experiments&& Hand calculations &&&&\\
Standard data&& Packages  && Copy \& paste &&\\
Search engines&& SPSS etc & \bigBracket & and edit data && Final paper\\
Literature &&  Graphics packages  && (text, images, graphs, etc)&&\\
Sensors && Specially-written code && into paper &&\\ 
\vdots && \vdots &&&& \\ &&&&&&\\ \hline
\end{tabular}\end{center}
\caption{A simplified schematic of the publication pipeline. The schematic shows a linear pipeline; in general, there will be much iteration and refinement. The RAP and \RAPstar\ approaches encode the manual steps of the pipeline processes so that they can be run automatically, and hence reproduce the results that underpin the final paper.}
\label{fig-pipeline}
\end{figure*}

For simplicity, the schematic pipeline in figure \ref{fig-pipeline} omits showing many standard steps in the creative scientific process: each step is iterated and modified as the research progresses, and as referees require revisions. The point illustrated, however, is that in typical scientific practice each arrowed step in the diagram leading to a published paper is largely or entirely manual, typically selecting and copying output from the previous phase, and then pasting the results into the next. The pipeline steps of data $\rightarrow\cdots\rightarrow$ paper is then repeatedly run by hand as the various components are refined and improved until the authors are happy with the paper. 

Different data is selected; calculations and analyses are modified; programs are debugged. As problems are detected in the paper, the data, calculations and programs are reviewed and refined. The process is rarely systematic, and even less likely to be documented --- after all, the atomic steps are trivial copy and paste actions. The final paper and the ideas it embodies are all that matters.

The basic insight of the reproducible analytic pipelines (RAP) proponents is that every time any atomic step in the pipeline process is performed it could have been automated \cite{rap,goldacre,turing-way}. If automated, it could then be repeated reliably --- unlike a manual cut and paste which is error-prone (in different ways!)\ every time it is done. In particular, when a process is automated, any other researcher, whether part of the authorship team or a later reader of the paper, can reproduce it reliably. It can be repeated if any experimental data, literature, or other knowledge changes, and the paper's analysis brought up to date with ease.

For example, if the paper in question is a systematic review, in principle it could be kept current ``just'' by automatically re-running the programmed atomic actions that it was built with in the first place. Indeed, this ability is one of the original motivations of RAP: Government agencies can produce up to date reports on request without having to repeat all the manual work and risk making procedural errors in doing so. Each time they do so, the RAP pipeline is reviewed and improved, so the quality of the reproduced work improves --- unlike in a non-RAP process where new errors are generally introduced.

RAP embodies Donald Knuth's comment, 

\begin{quote}\raggedright
{\setbox0=\hbox{``}
\hskip -\wd0\box0
Science is what we understand well enough to explain to a computer'' }
\\ \hfill from the foreword to $A=B$ \cite{a=b}
\end{quote}

The corollary is that if we are doing cut and paste that is arbitrary and cannot be programmed into a computer, then we are not doing science. Science is an algorithmic process, and therefore, as Knuth says, if we understand well enough what we are doing in science, we can explain it as programs, as code, for a computer to automate. That is RAP in a nutshell.

For example, in the present paper, we analyzed \the\dataN\ papers with \the\countAuthors\ authors, and one of the papers used a program that was composed out of \covidsimfiles\ files and had over \covidsimkLOC\ thousand lines of code. In the ``old days'' these numbers would have been manually worked out, then read and typed up by one or more of the authors. This is a potential source of error. As the analysis is extended the numbers may well change, and the authors would have to stay aware that a number like \the\dataN\ will need checking and updating over the period the paper is being written. It is an error-prone process, and has to be regularly repeated. Worse, other researchers may have no idea how the numbers were generated --- the paper is not fully reproducible. (More generally than simple numbers, as illustrated in this paragraph, papers may also have tables, diagrams, plots, and other types of result generated during the research.) Instead, in this paper, all those numbers (and many more) were computed automatically and change automatically if the data changes, and they were then inserted into the text of this typeset paper automatically. The figures are very probably correct.

Once processes in the pipeline are automated, this means that there is code to can run those steps again. Once there is code, it can be managed in a version control system. A version control system then provides an audit trail for free, as well as many advantages such as being able to backtrack to an earlier version to undo now-unwanted edits. Importantly, the automatic code can perform sanity checks on the process --- a very simple example is automatic bibliography systems that check that journal names, DOIs are correct and that references are correctly numbered, and so forth. They also allow the bibliographic data to be pooled and curated with other scientists, which improves its scope and quality.

Many systems provide tools to do this. GitHub (which is mentioned throughout this paper) provides \emph{actions\/}, which are named specifications that run workflows. GitHub happens to specify actions in the language Yaml, which, being a textual notation, in turn means that all the helpful features of GitHub --- open source, version control, etc --- can be applied to these pipelined processes as well. Helpful pipelines can be documented, shared and improved with open collaboration.

In the limit, almost the entire scientific process can be automated (and its interaction with the world automated with robots). There are many ways to do this; for example, the mathematical programming system \emph{Mathematica\/} makes the analysis of the data and the calculations and the paper ``the same thing'' in its integrated notebook user interface --- which behaves a bit like Microsoft Word, except that formulas can be evaluated and plotted, etc, with ease. Alternatives include R Markdown, an approach based in the open mathematical system R, many variations of literate programming \cite{relit}, and language-independent notebook systems like JupyterLab.

In all such systems, rerunning calculations re-creates the paper. Once \emph{Mathematica\/} (or which other system is used) has been set up, there is no repeated tinkering and error-prone copy and paste. Indeed, every time the system is run, the authors are likely to double-check the results after they have been re-computed --- so the RAP process actively helps reduce errors.

But a final paper is not the only product scientific authors create and publish. In computational modeling, they also often create the code and data that generates results for their paper, and both are often made available. Here is a critical insight: the code, just like the paper, contains text (source code and documentation) and data (e.g., constants) which have been copied and pasted from elsewhere. In more sophisticated RAPs, then, the coding process itself is fully part of the pipeline.

Software engineers have many tools for automatic code development (such as Unix's \texttt{make}) but the idea that these tools can be used to integrate and help automate code authoring as well as its documentation \emph{and\/} paper authoring is radical. Amongst other things, it means that the \emph{entire\/} research and development process of the paper (as well as all its underlying code) can be reproduced by others. Since this view has not been emphasized previously, we shall call it \RAPstar\ --- and we hope people will ask what that term means. The present paper is a modest example of \RAPstarp, the details of which are described more fully in the \supplement. Note that as \RAPstar\ objectifies how science is done, to a standard sufficient to enable a computer to run, enables it to be done better. \RAPstar\ means the processes of science become explicit code, which can then be scrutinized, optimized, and ensured correct by standard Software Engineering practices --- thus using computational ideas to improve science, not just to support it in the background.

The ideals and ideas of \RAPstar\ are starting to be more widely recognized, if not coherently integrated under that name. For example, the Executable Paper Grand Challenge Workshop \cite{Executable-Paper} explored the benefits of running scientific papers \emph{as\/} programs. 

\begin{change}
\subsection{The scientific emphasis on data}
Data has been at the center of science, certainly since the earliest days of astronomy collecting planetary and other information. Today it is widely recognized that lack of accessible and usable data that has already been collected limits the progress of science. Low quality data and poor access to data causes reproducibility problems, an increasingly recognized problem --- in 2015 it was estimated that \$28B a year is spent on preclinical research alone that is not reproducible \cite{preclinical-reproducibility}.

Curating data is taken seriously as a part of normal science and peer reviewed publication. Journal policies widely require appropriate discussion of data, much like they require appropriate discussion of statistics. Journals often require archiving data in standard formats so it can be accessed for reproduction in further scientific work. 

There are many current activities to proceduralize and standardize the more effective curation and use of data, such as the FAIR principles (Findable, Accessible, Interoperable and Reusable --- both for machines and for people) for scientific data management and stewardship \cite{fair}, and in the development of journal and national funder policies. For example, the 2022 update to the US National Institutes of Health data (not code) policies \cite{nih-policy} is described as a ``seismic mandate'' by \emph{Nature\/} \cite{nih-nature} in its attempt to improve reproducibility and open science. The RAP projects cited above are further examples.

On the whole, these cost estimates and initiatives under-play the role of code itself as a critical part: code has become the new laboratory for almost all science. The role of code specifically in modeling is discussed throughout this paper; without bespoke code, proposed models (unless very abstract) cannot make a quantifiable contribution to the literature. More generally, much data is embedded in code, and in the limit code and data are indistinguishable (see \supplement). Furthermore, code has additional problems of versions and compatibility beyond those of data, for example suitable compilers to run old code may no longer be available, and --- worse --- programming systems may silently produce different results when used on different computers. 

In general, without proper management of code --- for example to detect and report version control differences --- sharing code may even be counter-productive.\footnote{The data and code shared with the present paper includes cryptographic checksums; if somebody reproducing the work described here does not obtain the same checksums at least when they start their work, then there are problems that need investigation before relying on the reproducibility of the data.}

\subsection{The deceptive simplicity of code}\label{deceptive-simplicity-of-code}
It is a misconception that programming is easy and even children can do it \cite{fixit}. More correctly, toy programming is easy, but real programming is very difficult.

An analogy helps. Building houses is very easy --- indeed, many of us have built toy Lego houses. Obviously, though, a Lego house is not a \emph{real\/} house. It is not large enough or strong enough for safe human habitation! This point is obvious because we can see Lego houses. 

In contrast to Lego, computer programs are generally invisible, and therefore the engineering problems within them are also invisible. Thus the ``programming is easy'' clich\' e is deceptive --- programming appears easy \emph{because\/} professional standards of building software are ignored, because people cannot see the reasons why they are needed, because --- like Lego --- toy programs can look good but be unreliable, difficult to use or even dangerous. Thinking programming is easy is like sincerely appreciating a child's Lego building because we are not worried about subsidence, load bearing, electric shock, fire risks, water ingress, or even planning regulations. These are invisible professional issues that Lego builders ignore. Certainly, even real building is much easier and faster when the technical details are ignored, as anyone who has experienced a cowboy build may attest.

Unlike building (the Code of Hammurabi dates to around 1755{\sc bc}), programming is a very new discipline, and the problems of poor programming are not widely appreciated. Relevant professional standards are not enforced. Problems for the reliability of science arise when legitimate doodling with software drifts into claiming scientific results that do not have the reliable engineering structures underpinning them, let alone the properly developed and documented archived code, to justify them. In many countries, you cannot even start to build without first having plans approved, but who writes plans for software?

Since programming appears to be so easy, developing code has low status in scientific practice. Developers of code are rarely acknowledged in scientific papers. The implicit reasoning is: if programming is easy, then its intellectual contribution to science is negligible, so it is not even worth citing it or acknowledging the technical contributors to it. While this view prevails, the vicious cycle is that the low status means software development is done casually, which reinforces the low status.

In reaction to this vicious cycle, there is a growing movement to cite code correctly \cite{cite-code}, because code \emph{is\/} important, particularly for reproduction, testing and extension of any scientific work. 

Few journals editorial policies realize that data and code are theoretically and in practice indistinguishable (see \supplement). Given that data and code are equivalent and interchangeable, it follows that publishing policies on data handling should also apply at least as strictly to code.

\subsection{The central role of code is ignored}\label{central-role-of-code}
It is important that experiments and analysis, such as statistical analysis, are performed reliably and ethically. There are many protocols and journal policies that enforce good practice, for example journals often require adherence to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) \cite{prisma} for any paper performing a systematic review of the literature. Yet PRISMA, like many policies, ignores the role of computers, and ignores the software engineering principles that assure computation is reliable and reliably reported. 

PRISMA ``was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found,'' which sounds reasonable enough. PRISMA covers the review process carefully. For example, the authors should report the number of papers they included in their review. Perhaps $N=1093$. This number is then written into their draft paper, likely in several places. As the authors reads and revises their paper, submit it and respond to peer reviewers, it is likely that the number of papers included in the survey changes, or other details may change too. The authors now have a maintenance problem: where are the numbers that have changed, and what should they now be changed to? Doing a search-and-replace, whether automated or by hand, is fraught with difficulties. What happens if $1093$ is used for some other purposes as well? What happens if some of the $1093$ values were written as $1,093$ and are not noticed? Then there are the Human Factors, where slips and errors will happen in this process anyway. Similar iterative revision cycles happen with any paper, not just with systematic reviews. Typos, slips during cut-and-paste, and other errors are common. They are problems across all science.

If a computer program is involved in the process (it generally is for a systematic review) then the value of $N$ could easily be stored in a file where the paper typesetting process can access it. For example, if \LaTeX\ is the system of choice, the analysis could generate, say, 

\begin{center}\texttt{$\backslash$newcommand\{$\backslash$N-papers-reviewed\}\{1093\}}\end{center}

so that when and wherever the author writes \texttt{$\backslash$N-papers-reviewed} in their paper's text, the typeset paper says 1093 or whatever the correct value happens to be at the time. If so, then \emph{whenever\/} the survey is updated, the value cited in the paper is \emph{correctly\/} updated without any further intervention from the author. 

In general, not just PRISMA and not just numbers, but any data, text, graphs or tables, etc, can be reliably inserted into a paper automatically. 

PRISMA says nothing about how to ensure the final results of a survey are correctly and reliably presented in a paper, despite this being one of PRISMA's explicit motivations. Ironically, many journal publishing policies. Such rules reinforce the fallacy that code is trivial and unimportant. 
\end{change}

\subsection{Bugs, code and programming}\label{knowledge}
Critiques of data and model assumptions are increasingly common \cite{critiques,diagnosis-reviews} but program code is rarely mentioned. Yet data and program are formally equivalent (see \supplement, section \ref{on-code-data-publication}). Program code has as great an affect on results as the data; in fact, without code, the data would be uninterpreted and probably useless. Code, however, is harder to scrutinize, which means that errors in code have subtle often unnoticed effects on results.

Almost all code contains ``magic numbers'' --- that is, data masquerading as code. This common practice ensures that published data is very rarely all of the data because it omits the magic numbers embedded in the code. Data is often ``hard coded'' in programs with no explicit representation. Such issues emphasize the need for repositories to require the inclusion of code so all data, including that embedded in the code, is actually available. 

Bugs can be understood as discrepancies between what code is ought to do and what it actually does. Many bugs cause erroneous results, but bugs may be ``fail safe'' by causing a program to crash so no incorrect result can be delivered. Better, contracts and assertions are essential defensive programming technique that block compilation or block execution with incorrect results; they turn bugs into safe termination. None of the code examined for this paper includes any such techniques. 

If code is not documented it cannot be clear what it is intended to do, so it is not possible to detect and eliminate bugs. Indeed, even with good documentation, \emph{intentional bugs\/} will remain, that is, code that correctly implements the wrong things \cite{essence-of-software,fixit} --- they are bugs that were intended but were ideas based on mistaken ideas (students and inexperienced programmers make intentional bugs all the time). For instance, in numerical modeling, using an inappropriate method can introduce errors that are not ``bugs'' in the narrow sense of incorrectly implementing what was wanted (e.g., ill-conditioning), but are bugs in the wider sense of producing incorrect results --- that is, what was intended was wrong. 

Random numbers are widely used in computational science, e.g., for simulation or for randomizing experiments. Misuse of random numbers (e.g., using standard libraries without testing them) is a common cause of bugs \cite{knuth}.

\section{State of the art in computational modeling}
\label{section-pandemic-modeling}
A review of epidemic modeling \cite{science-review} says, ``we use the words `computational modelling' loosely,'' and then, curiously, the review discusses exclusively mathematical modeling, implying that for the authors, and for the peer reviewers, there is no conscious role for code or computation as such. It appears that the new insights, advances, rigor, and problems that computers bring to research are not considered relevant. 

A systematic review \cite{diagnosis-reviews} of published COVID models for individual diagnosis and prognosis in clinical care, including apps and online tools, noted the common failure to follow standard TRIPOD guidelines \cite{tripod}. (TRIPOD guidelines ignore code completely.) The review \cite{diagnosis-reviews} ignored the mapping from models to their implementation, yet if code is unreliable, the model \emph{cannot\/} be reliably used, and cannot be reliably interpreted. It should be noted that flowcharts, which the review did consider, are programs intended for direct human use. Flowcharts, too, should be designed as carefully as code, for exactly the same reason: it is hard to program reliably. 

A high-profile 2020 COVID-19 model \cite{nature-summary,ICmodel} uses a modified 2005 computer program \cite{avianFluModel,originalICmodel} for H5N1 in Thailand; it did not model air travel or other factors required for later western COVID-19 modeling. The 2020 model forms part of a series of papers \cite{ICmodel,avianFluModel,originalICmodel} none of which provide details of their code. 

A co-author disclosed \cite{tweet} that the code was thousands of lines long and was undocumented C code. As Ferguson, the original code author, noted in an interview, 

\begin{quote}\raggedright
\setbox0=\hbox{``}
\hskip -\wd0\box0 
For me the code is not a mess, but it's all in my head, completely undocumented. Nobody would be able to use it~\ldots
'' \cite{ferguson-interview}\end{quote}

This comment was made by a respected, influential world-leading scientist, with many peer-reviewed publications, and a respectable $h$-index\footnote{$h$-index: the largest value of $h$ such that at least $h$ papers by the author have each been cited at least $h$ times. The figure cited for Ferguson was obtained from Google Scholar on 20 January 2022. (Typical $h$ values vary by discipline.)} of 93. Ferguson should be well aware of the standards of coding used in at least his own field. This comment, quoted above, is therefore likely to be representative of the standards of the field as a whole.

Ferguson's admission is tantamount to saying that the published scientific findings are and need not reproducible.\footnote{A constructive discussion of software engineering approaches to reproducibility can be found in \cite{basic-reproducibilty}.} 

Lack of reproducibility is problematic, especially as the code would have required many non-trivial modifications to update it for COVID-19 with its different assumptions; moreover, the code would have had to have been updated very rapidly in response to the urgent COVID-19 crisis. 

If Ferguson's C code had been made available for review, the reviewers would not have known how to evaluate it without the relevant documentation. It is, in fact, hard to imagine how a large undocumented program could have been repeatedly modified over fifteen years without becoming incoherent. If code is undocumented, there would be an understandable temptation to modify it arbitrarily to get desired results; worse, without documentation and proper commenting, it is methodologically impossible to distinguish legitimate attempts at debugging from merely fudging the results. In contrast, if code is properly documented, the documentation defines the original intentions (including formally using mathematics to do so), and therefore any modifications will need to be justified and explained --- or the theory revised.

The programming language C which was used is not a dependable language; to develop reliable code in C requires professional tools and skills. Moreover, C code is not portable, which limits making it available for other scientists to use safely: C notoriously gets different results with different compliers, libraries, or hardware. In fact, in any area where reliable programming is required in a C-like language, a special dialect such as MISRA C is used, which manages the serious design flaws of C that otherwise make it too unreliable \cite{misra}. The \supplement\ discusses these issues further.

Ferguson, author of the code, says of the criticisms of his code, 

\begin{quote}\raggedright
\setbox0=\hbox{``}
\hskip -\wd0\box0 
However, none of the criticisms of the code affects the mathematics or science of the simulation'' \cite{thumbs-up}
\end{quote}

Really? The \emph{prior\/} theoretical epidemiology may be fine if it does not use any of his code, but if the science is not supported by code that correctly implements the models, then the program's output cannot be relied on without independent evidence. Typically, the models would be developed iteratively as their results are improved to better fit the paper's goals --- but this, especially when it is done by tinkering, as here --- risks making the code arbitrarily fit the goals (like regression over-fitting), rather than to objectively elucidate the science. In fact, the Ferguson computational model is large,\footnote{Ferguson's \texttt{covid-sim} system is \covidsimkLOC\ kLOC (thousands of lines of code), composed of \covidsimfiles\ files, and uses \covidsimdata\ of data. It is now rewritten from C into C++ with Python, R, sh, YML/JSON, etc. For more details, see \supplement.}
 so it is implausible that ``mathematics or science'' has been correctly implemented in them. Therefore Ferguson's \emph{reported\/} science cannot be reliable. Getting the science right, which depends on correct code, is a normal requirement of \emph{reproducibility}.

The code in \cite{nature-summary,ICmodel} has been ``reproduced,'' as reported in \emph{Nature\/} \cite{codecheck,thumbs-up}, but this so-called reproduction merely confirmed the code could be run again and produced comparable results (compared, apparently, to an Excel spreadsheet!). That can be achieved at this low level of sophistication is hardly surprising, regardless of the quality of the code. There was no scientific insight that merits the use of the word ``reproduction.'' If reproducibility is to be a useful scientific criterion, an \emph{independently\/} developed model needs to produce equivalent results (called $N$-version programming, a standard software engineering practice \cite{NVP}) like public health surely requires --- as, indeed, Ferguson's own influenza paper \cite{nvp-ferguson} argues. Meanwhile, it is a shame that using software for scientific models has enabled the bar to reproducibility, as understood by journals such as \emph{Nature}, to be lowered to a mechanical level that is only sufficient to detect some forms of dishonesty, as opposed to methodological limitations, which is the point of reproducing work.

Because of the recognized importance of the Ferguson paper, a project started to document its code  \cite{refactoring}.\footnote{The system is open source, available at \url{github.com/mrc-ide/covid-sim} version (19 July 2021).} Documenting the code now in hindsight, even if done rigorously, may describe what it does, \emph{including\/} its bugs, but it is unlikely to explain what it was originally intended to have done. As the code is documented, bugs will be found, which will then be fixed (refactoring), and so the belatedly-documented code will not be the code that was used in the published models; it will be different. It is well-known that documenting code helps improve it, so it is surprising to find an undocumented model being used in the first place. The revised code has now been published, and it has been heavily criticized \citeeg{bad-code}, supporting the concerns expressed in the present paper.

Some epidemiology papers \citeeg{pseudo} publish models in pseudo-code, a simplified form of programming. Pseudo-code looks deceptively like real code that might be copied to try to reproduce it, but pseudo-code introduces invisible and unknown simplifications. Pseudo-code, properly used, can give a helpful impression of the overall approach of an algorithm, certainly, but pseudo-code alone is not a surrogate for code: using it is arguably even worse than not publishing code at all. Pseudo-code is not precise enough to help anyone scrutinize a model; copying pseudo-code introduces bugs. An extensive criticism of pseudo-code, and discussion of tools for reliable publication of code can be found elsewhere \cite{relit}. {The \supplement\ provides further discussion of reproducibility.}

\subsection{Computational science beyond pandemic modeling}
\label{section-science-beyond-pandemic-modeling}
\begin{change}
Epidemiology has a high profile because of the current COVID pandemic, but the problems of unreliable code are not limited to COVID-19 modeling papers, which, understandably, were perhaps rushed into publication. For instance, a 2009 paper reporting a model of H5N1 pandemic mitigation strategies \cite{flu-model} provides no details of its code. Its \supplement, which might have provided code, no longer exists.

There are many other areas of computational science that are equally if not more critical, and many will have longer-lasting impact. Climate change modeling is one such example that will have an impact long beyond the COVID pandemic.

A short 2022 summary of typical problems of software engineering impacting science generally appears in \emph{Nature\/} \cite{nature-review}, describing diverse and sometimes persistent problems encountered during research in cognitive neuroscience, psychology, chemistry, nuclear magnetic resonance, mechanical and aerospace engineering, genomics, oceanography, and in migration. The paper \cite{nature-review}  makes some misleading comments about the simplicity of software engineering, e.g., ``If code cannot be bug-free, it can at least be developed so that any bugs are relatively easy to find.''

Guest and Martin \cite{psychological-modeling} in another 2022 paper promote the use of computational modeling, arguing that through writing code, one debugs scientific thinking. Psychology, Guest and Martin's focus, has an interesting relationship with software, as computational models are often used to model cognition and to compare results with human (or animal) experiments \cite{psychological-modeling}. In this field, the computation does not just generate results, but is used to explicitly explore the assumptions and structures of the scientific frameworks from which the models are derived. Computational models can even be used to perform experiments that would be unethical on live participants, for instance involving lesioning (damaging) artificial neural networks. It should be noted that such use of cognitive models is controversial --- on the one hand, the software allows experiments to be (apparently) precisely specified and reproduced, but on the other hand in their quest for psychological realism the models themselves have become very complex and it is no longer clear what the science is precisely (e.g., ACT-R, one widely-used theory for simulating and understanding human cognition, has been under development since 1973 and is now a 120 kLOC Common LISP and Python system \cite{actr}; and of course any paper using ACT-R would require additional code on top of the basic framework).

The psychology paper \cite{psychological-modeling} demonstrates building an example computational model from scratch to illustrate their own framework of computational science. In fact their example model has no psychological content: a simple numerical test is performed, but the psychology of why the result is counterintuitive --- the psychological content --- is not explored. Be that as it may, they develop a mathematical specification and discuss a short Python program they claim implements it. 

The Python code is presented without derivation or discussion, as if software engineering is trivial. The program listed in the paper certainly runs without obvious problems (ignoring typographical errors due to the journal's publishers), but ironically the Python does not implement the mathematical specification explicitly provided for it,\footnote{\begin{change}More precisely: the program has a bug, and/or the specification given is wrong or too abstract.\end{change}} thus unintentionally undermining the argument of the paper. 

One might argue the bug is trivial (the program prints \texttt{False} when it should print \texttt{b}), but to dismiss such a bug would be comparable to dismissing a statistical error that says $p=\mbox{\tt False}$ which would be nonsense --- if a program printed that, one would be justified in suspecting the quality of the entire program and its analyses. Inadvertently, it would seem, then, that the paper shows that just writing code does not help debug scientific thinking: instead, code must first be derived in a rigorous way and actually be correct (at least when finished). Otherwise, computational modeling with inadequate software engineering will very likely introduce errors into scientific thinking.
\end{change}

Code generally for any field of scientific modeling needs to be carefully documented and explained because all code has tacit assumptions, bugs and cybersecurity vulnerabilities \cite{Ben,nature-review,se-bias} that, if not articulated \emph{and properly managed}, can affect results in unknown ways that may undermine any claims. People reading the code will not know how to obtain good, let alone better results, because they do not know exactly what was intended in the first place. The problem is analogous to the problem of failing to elaborate statistical claims properly: failure to do so suggests that the claims may have unknown limitations or even downright flaws.

Even good quality code has, on average, a defect every 100 lines --- and {such a low} rate is only achieved by experienced industrial software developers \cite{ourReview}. World-class software can attain maybe 1 bug per 1,000 lines of code. Code developed for experimental research purposes will have higher rates of bugs than professional industrial software, because the code is less well-defined and evolves as the researchers gain new insights into their models. In addition, and perhaps more widely recognized, code --- especially but not exclusively mathematical code --- can be subject to numerical errors \cite{hamming}. It is therefore inevitable that typical modeling code has many bugs (reference \cite{NVP} is a slightly-dated but very insightful discussion). Such bugs undermine confidence in model results. 

Only if there is access to the \emph{actual\/} code and data (in the specific version that was used for preparing the paper) does anyone know what researchers have done, but merely making code available (for instance, providing it in their \supplement\ with papers, putting it in repositories, or using open source) is not sufficient. It is noteworthy that some papers disclosed that they had access to special hardware.

Some COVID-19 papers \citeeg{unfinished} make unfinished, incomplete code available. While some \citeeg{unfinished,lancet-unfinished} do make what they call ``documented'' code available, they provide no more than superficial comments. This is \emph{not\/} documentation as properly understood. Such comments do not explain code, explain contracts, nor explain algorithms. Some readers of the present paper may not recognize these technical software terms; contracts, for instance, originated in work in the 1960s \cite{hoare}, and are now well-established practice in reliable programming {(see the \supplement\ for a checklist of many relevant, professional software engineering concepts and resources)}.

If full code is made available, it may be technically ``reproducible,'' but the scientific point is to be able to understand and challenge, potentially refute, the findings; to do that, much more is required than merely being able to run the code \cite{notebooks,popper}.

Even if a computer can run it, badly-written code (as found in \emph{all\/} the research reviewed in the present paper {and indeed in computer science research itself \cite{machine-learning-reproducibility}}) is inscrutable. Only if there is access to \emph{adequate\/} documentation can anyone know what the researchers \emph{intended\/} to do. Without all three (code, data, adequate documentation), there are dangers that a paper simplifies or exaggerates the results reported, and that omissions, bugs and errors in the code or data, generally unnoticed by the paper's authors and reviewers, will have affected the results they report \cite{relit}. 

Making outline code (including pseudo-code) available without proper documentation and without discussing its limitations is unethical: it encourages others to reproduce and build on poor work. 

This paper's \supplement\ describes a pilot survey of papers covering a broad range of computational science published in leading journals. The quality of the surveyed computational science was no better than the specialist fields of computational science described above. No papers provided any evidence their code was adequately tested or rigorously developed; none used methodologies like RAP or \RAPstarp; only one paper discussed any relevant software engineering methods (independent coding). Although in the sample 81\% of papers were published in journals that had code policies (which themselves are weak), and 42\% of papers in those journals breached their own code policies.

\section{Improving the computational sciences}
\label{section-discussion}
\begin{change}
Despite its critical contribution to science, effective use or access to quality code is not routine. 
\end{change} 
Using structured repositories that provide suggestions for and which encourage good practice (such as Dryad and GitHub), and requiring their use, would be a lever to improve the quality and value of code and documentation in published papers. The evidence suggests that, generally, some but not all manually developed code is uploaded to a repository just before submitting the paper in order to ``go through the motions.'' In the surveyed papers there is no evidence (over the survey sample) that any published code was maintained using the repositories. This is consistent with finished code being uploaded to a repository for the purposes of satisfying publishing requirements, but not being maintained in a repository earlier nor later.\footnote{Using a standard repository for lodging a paper's supporting code helps other scientists access the code easily, but not using the repository for developing and maintaining the code means the author of the paper misses out on many helpful features of repositories, such as version control, open source development and review, actions and other approaches for automating \RAPstarp, and so on --- depending on the repository features available.}

\subsection{Emphasizing code, and the importance of correct code}
There is widespread appreciation of data and its role in modern science, but, strangely, the role of \emph{correct\/} code is not appreciated. Data, at least on the scale typical of modern science, is useless without correct code.

The well-motivated RAP (reproducible analytic pathways) movement needs to extend its concerns to cover reliable code, as discussed in section \ref{RAP-section}. Without reliable code, no computational science --- and no modern science --- can be considered reliable. Software Engineering is the professional and scientific field that ensures reliable code; without adopting appropriate techniques from Software Engineering, code is not reliable. 

Furthermore, this paper suggested \RAPstar\ to emphasize the additional critical role of automating code development in reliable science. In particular, the heuristics of RAP (e.g., if part of a research process is being done manually, it should be automated) also, as generalized by \RAPstarp, apply to code and its documentation and scripting, not just to end results like reports and papers. Software Engineers will recognize this heuristic as an encouragement to use software tools.

\subsection{A call to action}\label{summary}
Computer programs are the laboratories of modern scientists, and should be used with a comparable level of care that virologists use in their laboratories --- lab books and all \cite{notebooks} --- and for the same reasons: computer bugs accidentally cultured in one laboratory can infect research and policy worldwide. Given the urgency of rigorously understanding COVID-19, any epidemic for that matter, it is essential that epidemiologists engage professional software engineers to help develop reliable laboratory methodologies. For instance, code lab books can be generated and signed easily.

Software used for informing public health policy, medical research or other medical applications is \emph{critical software}. Professional critical software development, as used in aviation and the nuclear power industry, is (put briefly) based on \emph{correct by construction}: \cite{cbc} effectively, design it right first time, {supported by numerous rigorous techniques, such as formal methods, to manage error. (See extensive discussion in this paper's \supplement.)}\ Not coincidentally, these are \emph{exactly\/} the right methods to ensure code is both dependable and scrutable. Conversely, not following these practices undermines the rigor of the science.

An analogous situation arises in ethics. There are some sorts of research that are ethically unacceptable, but few people have the objectivity and ethical expertise to make sound ethical judgements, particularly when it comes to assessing their own work. Misuse of data, exploiting vulnerable people, and not obtaining informed consent are typical ethical problems. National funders, and others, therefore require Ethics Boards to formally review ethical quality. Medical journals will not publish research that has not undergone appropriate ethical review. 

Analogously, and supplementing Ethics Boards, Software Engineering Boards would authorize as well as provide advice to guide the implementation of high-quality software engineering. Just as journals require conflicts of interest statements, data availability statements, and ethics board clearance, we should move to epidemic modeling papers --- and in due course, all scientific papers --- being required to include Software Engineering Board clearance statements as appropriate. {Software Engineers have a code of ethics that applies to \emph{their\/} work in epidemic modeling \cite{ethics-code}.}

{The present paper did not explore data, because in almost all cases the code and data were explained so poorly and archived so haphazardly it would be impossible to know whether the author's intentions were being followed.\footnote{{For the present paper, all the code, data, analysis and documents are available for download in a single zip file.}} Some journals have policies that code is available (see the \supplement), but they should require that code is not just available in principle but \emph{actually works\/} on the relevant data. Ideally, the authors should test a clean deployed build of their code and save the results. Presumably a paper's authors must have run their code successfully on \emph{some\/} data (if any, but see section \ref{on-code-data-publication} in the \supplement) at least once, so preparing the code and data in a way that is reproducible should be a routine and uncontentious part of the rigorous development of  code underpinning \emph{any\/} scientific claims. This requirement is no more unreasonable than requesting good statistics, as discussed in the opening of the paper. And the solution is the same: relevant experts --- whether statisticians or software engineers --- need to be routinely available and engaged with the science. SEBs would be a straight forward way of helping achieve this.}

There need to be many Software Engineering Boards (SEBs) to ensure convenient access and oversight, potentially at least one per university. Active, professional software engineers should be on these SEBs; this is not a job for people who are not qualified and experienced in the area or who are not actively connected with the true state of the art. There are many high-quality software companies (especially those in safety-critical areas like aviation and nuclear power) who would be willing and competent to help.

Open Source generally improves the quality of software. SEBs will take account of the fact that open source software enables contributors to be located anywhere, typically without a formal contractual relationship with the leading researchers. Where appropriate, then, SEBs might require \emph{local\/} version control, unit testing, static analysis {and other quality control methods for which the lead scientist and software engineer remain responsible, and may even need to sign off (and certainly be signed off by the SEB\@).}\ {Software engineering publishers are already developing rigorous badging initiatives to indicate the level of formal review of the quality of software for use in peer reviewed publications \cite{acm-artifacts}.}\ {See this paper's \supplement\ for further suggestions.}

A potential argument against SEBs is that they may become onerous, onerous to run and onerous to comply with their requirements. A more mature view is that SEBs need their processes to be adaptable and proportionate. If software being developed is of low risk, then less stringent engineering is required than if the software could cause frequent and critical outcomes, say in their impact on public health policy for a nation. Hence SEBs processes are likely to follow a risk analysis, perhaps starting with a simple checklist. {There are standard ways to do this, such as following IEC 61508:2010 \cite{redmill,iec61508} or similar. Following a risk analysis (based on safety assurance cases, controlled documents and so on, if appropriate to the domain), the Board would focus scrutiny where it is beneficial without obstructing routine science.}

A professional organization, such as the UK Royal Academy of Engineering ideally working in collaboration with other national international bodies such as IFIP, should be asked to develop and support a framework for SEBs. SEBs could be quickly established to provide direct access to mature software engineering expertise for both researchers and for journals seeking competent peer reviewers. In addition, particularly during a pandemic, SEBs would provide direct access to their expertise for Governments and public policy organizations. Given the urgency, this paper recommends that \emph{ad hoc\/} SEBs should be established for this purpose.

SEBs are a new suggestion, providing a supportive, collaborative process. Methodological suggestions already made in the literature include open source and specific software engineering methodologies to improve reproducibility \cite{basic-reproducibilty,open-source}. While \cite{ABCs-SE} provides an insightful framework to conceptualize approaches and compare their merits, there is clearly scope for much further research to provide an evidence base to motivate and assess appropriate interventions to help scientists do more rigorous and effective software engineering to support their research and publishing. These and further issues are discussed at greater length in the \supplement. 

\begin{change}
\subsection{Action must be mutual and interdisciplinary}
\setcounter{footnote}{1}
Code is only part of science, and only one critical factor in the wider reproducibility crisis: SEBs must work with --- and be engaged by --- other reproducibility initiatives.
 
Relying on Software Engineering Boards (SEBs) \emph{alone\/} would continue one of the current besetting problems about the role of code. The conventional view is that scientists do the hard work compared to the ``easy'' coding work\footnote{The programming is easy fallacy is refuted in sections \ref{deceptive-simplicity-of-code} \& \ref{central-role-of-code}.} so they just need to tell programmers what to do. This is the view expressed by Landauer in his classic book \emph{The Trouble with Computers\/} \cite{landauer,thimbleby-landauer}, where he argues that the trouble with computers, which he explores at some length, is that we need to spend more effort in working out what computers should do (i.e., do the science better) and then just tell programmers to do \emph{that}.  

On the contrary, competent software engineers have insights into the logic, coherence, complexity, and computability of what they are asked to do, and often that logic needs refining or optimizing. In other words, the software engineers can bring important insights back into the science, hence improving or changing the questions and assumptions the science relies on. This insight was widely recognized in the specialist area of numerical computation: ``here is a formula I want you to just code up''~\ldots\ ``but that is ill-conditioned, there is no good answer.'' Ideally, then, it is not just a sequential process of science $\rightarrow$ code $\rightarrow$ results, but an iterative cycle of collaboration and growing mutual understanding.

In short, the way SEBs work and are used is crucial to their success. Software engineers can help improve the science, so it is not just a matter of asking a SEB  whether some coding practices (like documentation) are satisfactory, but whether the SEB has insights into the science itself too. Most effectively, this requires interdisciplinary working practices (science plus software engineering) with mutual respect for their contributing expertise.

\subsection{SEBs could be enforced}
It is routine for funders and journals to require appropriate ethical processes and ethical statements, typically specifying data security and confidentiality, appropriate handling of vulnerable participants, consent, and so on. It would be easy for funders and journals to require equivalent types of statements on software engineering.

Journal policies could also start to explicitly encourage computationally reproducible science using RAP and \RAPstar\ techniques. As this paper's \supplement\ makes clear in its section \ref{restrictive-policies}, many journals (like \emph{PLOS ONE\/} and, ironically, \emph{IEEE Transactions on Software Engineering\/}) and repositories do not.

\subsection{The paper as a scientific laboratory}
RAP (figure \ref{fig-pipeline} and section \ref{RAP-section}) recognizes that much of doing science is, to the extent that it is reproducible, a proceduralized, algorithmically-framed process: many scientific processes, most obviously in computational science, can be automated and reproduced at will (at least when they are done to high enough Software Engineering standards). \RAPstar\ further recognizes that the codification of science is itself a scientific object. That is, the code recording the scientific pipeline is not just scaffolding to support a scientific project, but is in itself a scientific product. It can be criticized, debugged, optimized, refactored, generalized into a virtual machine --- code --- that can do any related type of science. Written well, the code is an explanation of how particular science is done. 

In other words, as a paper is developed the computational contributions do not just generate results, but they objectify the scientific processes that have generated the results: they make doing science a first class object that can be seen, scrutinized and thought about --- it is code, after all --- generalized and applied more widely. In computer science terms, the paper (paper, data, code, etc) becomes a first-class object; that is, it has all the rights and operations as available to any other object. In scientific terms, then, it can be controlled, measured, stored, mixed, split, searched, analyzed, written-up, and so forth as an object of proper scientific enquiry. This is trivially true in the field of Computer Science, since (for many papers) the object of study \emph{is\/} a computer program --- the only debate is how much of that code is literally in the paper (or downloadable from it) or just talked about in the paper.

Code thus becomes a tool reifying scientific insights, which therefore (if of adequate quality) itself merits publication, so others can reproduce, generalize and progress the science confidently themselves. The scientific paper turns from being a rhetorical record of some scientific investigation into also embodying the algorithms that performed the science. \RAPstar\ makes the algorithms explicit. The paper becomes the laboratory where the code is developed, tested and reasoned about. As the algorithms in code are explicit, science gains leverage and will advance more rapidly.

One of the many exciting opportunities is transforming how scientists code, how they turn computation from mere support into a keystone of thinking about their activities. 

Today, computational science papers typically have some mathematical equations to state and communicate their theories, but the results are generated by embarrassingly complex imperative programs that defy scrutiny, typically written in languages like C, Python, or Java. Increasingly, though, computational science is building on programming languages that can directly support science. In a language like \emph{Mathematica}, a scientist can program directly in (say) systems of differential equations, and there need be no difference at all between how ideas in the paper and the ideas in the code are expressed. In fact, the paper has become the scientist's laboratory, the place where they do research.

The more we view the paper as a scientific laboratory, the more we gain from the \RAPstar\ perspective and the more science will gain from improved, conceptually broadened computational science. The more we will want to mature Software Engineering standards too, because the quality of future science depends on them.

\subsection{Benefits beyond computational science}
Computational sciences recognize the key role of computation (if not sufficiently emphasizing the role of code), but many fields do not recognize computation as such and therefore they are missing out on the leverage that comes with recognizing computation as a first class player in their scientific activities. 

One example will suffice. A lot of healthcare is delivered by computers, yet medical papers remain traditional and only very exceptionally refer to or include code. However, much clinical practice relies on computer analysis, but inevitably it has to use code unrelated to the code developed by the scientists doing the primary research, as their code is neither peer reviewed nor published. Code is professionally invisible, has no prestige, and there is therefore no investment in studying or improving it. Conversely, the challenging critical issues (including patient safety) that require clinical code to be more reliable than scientific tinkering do not get evaluated by researchers. Certainly, the issues, if noticed,  would stimulate further science --- sexism and racism in healthcare algorithms being a case in point \cite{fixit}. 

Without taking the lessons of improved computational science to other fields, like medicine, there will continue to be an unfortunate and unnecessary disconnect between research and practice, and one where both science and practice suffer because code is not recognized as a contribution to science \cite{fixit}. 

\end{change}

\ignore{\subsection{Suggestions for further work}
Although this study of software engineering in scientific research (specifically, in peer reviewed publications) was necessarily interpretive in nature, the corpus of data (namely, the selected \the\dataN\ papers) was rigorously gathered so that it could be later updated or extended in size or scope. However, the insights appear to be general.

Further work to extend the scope of the survey beyond the basic requirements of the present paper is of course worthwhile, but the following cases (listed in the next few paragraphs) suggest that the problem is widespread. We argue, then, that we should be focusing effort on avoiding problems and considering proposed solutions (see section \ref{summary}), not just assessing the problems highlighted in this paper with increasing scale or rigor. }

\section{Conclusions}
We need to improve the quality of software engineering that supports science. While this paper was originally motivated by Ferguson's public statements \citeeg{tweet,ferguson-interview}, the wider evidence reviewed shows that current coding practice makes for poor science in many fields. In a pandemic, scientific modeling, such as epidemiological modeling, track and trace \cite{excel-fiasco}, modeling COVID mutation pressures against vaccine shortages and delays between vaccinations \cite{science-delays}, etc, drive public policy and have a direct impact on quality of life. Unfortunately, Software Engineering good practice, to help \emph{do\/} science has been absent.

The main challenges to mature computational scientific research are:

\begin{itemize}\raggedright
\item 
To manage software development to reduce the unnoticed and unknown impacts of bugs and poor programming practices that papers rely on. Computer code should be explicit, accessible (well-structured, etc), and properly documented. Papers should be explicit on their software methodologies, limitations and weaknesses, just as Whitty expressed more generally about the standards of science \cite{whitty}. Professional software methodologies should not be ignored.

\item 
To use computation to help make scientific processes explicit, so that they can be reproduced, scrutinized and improved. RAP is an increasingly popular way to help do some of this, but as this paper points out, RAP should be generalized to \RAPstar\ to help the computational parts of science as well, leading to a virtuous circle.
\end{itemize}

While programming seems easy, and is often taken for granted and done casually, programming \emph{well\/} is very difficult \cite{fixit}. We know from software research than ordinary programming is very buggy and unreliable. Without adequately specified and documented code and data, research is not open to scrutiny, let alone proper review, and its quality is suspect. Some have argued that availability of code and data ensure research is reproducible, but that is na\"\i ve criterion: computer programs are easy to run and reproduce results, but being able to reproduce something of low quality does not make it more reliable \cite{reproducibility,relit,popper}. 

Software Engineering Boards (as introduced in this paper) are a straight forward, constructive and practical way to support and improve computer-based science. This paper's \supplement\ summarizes the relevant professional software engineering practice that Software Engineering Boards would use, including discussing how and why software engineering helps improve code reliability, dependability, and quality.

\section*{Supporting information}
\newcount\csrefcount \csrefcount=0
\def\csref{\global\advance\csrefcount by 1}
\long\def\ethics#1{\paragraph*{Ethics}#1}
\long\def\ack#1{\paragraph*{Acknowledgments}#1}
\long\def\dataaccess#1{\paragraph*{Data and code access}#1}
\long\def\aucontribute#1{\paragraph*{Author contribution}#1}
\long\def\competing#1{\paragraph*{Competing interests}#1}
\long\def\funding#1{\paragraph*{Funding}#1}

\ack{The author is very grateful for comments from: \csref Ross Anderson, \csref Nicholas Beale, \csref Ann Blandford, \csref Paul Cairns, \csref Rod Chapman, \csref Jos\'e Corr\'ea de~S\`a, \csref Paul Curzon, \csref Jeremy Gibbons, \csref Richard Harvey, \csref Will Hawkins, \csref Ben Hocking, \csref Daniel Jackson, \csref Peter Ladkin, \csref Bev Littlewood, \csref Paolo Masci, Stephen Mason, \csref Robert Nachbar, \csref Martin Newby, \csref Patrick Oladimeji, \csref Claudia Pagliari, \csref Simon Robinson, \csref Jonathan Rowanhill, \csref John Rushby, \csref Susan Stepney, Prue Thimbleby, \csref Will Thimbleby, \csref Martyn Thomas, and \csref Ben Wilson provided very helpful comments.}
%\the\csrefcount\ computer science reviewers.

%\ethics{This article presents research with ethical considerations but does not fall within the usual scope of ethics policies.}

\dataaccess{There is an extended discussion of the methodology of this paper and its benefits in the \supplement, section \ref{on-code-data-publication}. The \supplement\ also presents all raw data in tabular form. All material is also available for download at \url{github.com/haroldthimbleby/Software-Enginering-Boards},\footnote{This is a temporary URL before meeting Royal Society journal publication repository requirements.} which has been tested in a clean build, etc. The data is encoded in JSON\@. JavaScript code, conveniently in the same file as the JSON data, checks (with \the\JSONerrorCount\ possible types of error report) and converts the JSON data into \LaTeX\ number registers and summary tables, etc, thus making it trivial to typeset all results reliably in this paper and in its \supplement\ \emph{directly\/} from the automatic data analysis. \begin{change}The \supplement\ describes how  journal publishing policies do not support such automatic approaches, undermining the benefits of \RAPstar\ and reliable reproducibility.\end{change}

In addition, a standard CSV file is generated from the JSON in case this is more convenient to use, for instance to browse directly in Excel or other compatible application.}

\aucontribute{Harold Thimbleby is the sole author. An preliminary outline of this paper, containing no supplementary material or data, was submitted to the UK Parliamentary Science and Technology Select Committee's inquiry into UK Science, Research and Technology Capability and Influence in Global Disease Outbreaks, under reference LAS905222, 7 April, 2020. The evidence, which was not peer reviewed and is only available after an explicit search, briefly summarizes the case for Software Engineering Boards, but without the detailed analysis and case studies of the literature, etc, that are in the present paper. It is available to the public \cite{parliamentary-evidence,my-parliamentary-evidence}.}

\competing{The author declares no competing interests.}

\funding{This work was jointly supported by See Change (M\&RA-P), Scotland (an anonymous funder), by the Engineering and Physical Sciences Research Council [grant EP/M022722/1], and by the Royal Academy of Engineering through the Engineering~X Pandemic Preparedness Programme [grant EXPP2021\textbackslash 1\textbackslash 186]. The funders had no involvement in the research or in this paper.}

%\nolinenumbers

{\raggedright

\bibliographystyle{mysiam}
\def\urlprefix{}
\initialiseBibliography{References}{1}{}
\bibliography{paper-seb-main.bib}

% Note that the supplementary material calculates how many references there are,
% because it reads in this .tex file's .aux file so that it can cite the references 
% above with the same number sequence. 
%
% It simple redefines \bibitem to work it out while it
% defines all the citations from the bibliography above.
%
% The same technique allows the supplementary material to have 2 more bibliographies,
% with consecutive numbering.
}

%\label{LastPage}
%save Latex register    {linenumber}
%\makeatletter
%\immediate\write\@auxout{\string\def\string\MainPaperLastLine{\arabic{linenumber}}}
%\makeatother
\end{document}

\def \allInOne {define this so we don't include header in supplemental material}
\input paper-seb-supplementary-material.tex
\end{document}

%\immediate\write\@auxout{\string \newcount \string \savedlastReferenceNumber
%\string \savedlastReferenceNumber=\the\savedlastReferenceNumber}

\end{document} 
